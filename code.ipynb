{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train.csv file from the dataset folder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "df = pd.read_csv('dataset/train.csv')\n",
    "print(df.head())\n",
    "\n",
    "#check for nan or non existent values in the dataset\n",
    "df.isnull().sum()\n",
    "\n",
    "#print unique countries in country column\n",
    "print(df['country'].unique())\n",
    "\n",
    "#Plot num_sold over time for each unique country in country column, the date column is in the format yyyy-mm-dd\n",
    "#convert the date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "#define unique countries\n",
    "countries = df['country'].unique()\n",
    "\n",
    "#plot num_sold over time for each unique country in its own subplot\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, country in enumerate(countries):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    df[df['country'] == country].groupby('date')['num_sold'].sum().plot()\n",
    "    plt.title(country)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('num_sold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique values in country, store and product columns, and min and max for date in num_sold column\n",
    "print(df['country'].unique())\n",
    "print(df['store'].unique())\n",
    "print(df['product'].unique())\n",
    "print(df['num_sold'].min(), df['num_sold'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each country, plot the num_sold over time for each store\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, country in enumerate(countries):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    for store in df['store'].unique():\n",
    "        df[(df['country'] == country) & (df['store'] == store)].groupby('date')['num_sold'].sum().plot()\n",
    "    plt.title(country)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('num_sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each country, plot the num_sold over time for each product\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, country in enumerate(countries):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    for product in df['product'].unique():\n",
    "        df[(df['country'] == country) & (df['product'] == product)].groupby('date')['num_sold'].sum().plot()\n",
    "    plt.title(country)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('num_sold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mean/Median prediction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Compute mean sales for each group\n",
    "mean_sales = train.groupby(['country', 'store', 'product'])['num_sold'].mean().reset_index()\n",
    "mean_sales.rename(columns={'num_sold': 'mean_num_sold'}, inplace=True)\n",
    "\n",
    "# Merge mean sales with test data\n",
    "test = test.merge(mean_sales, on=['country', 'store', 'product'], how='left')\n",
    "\n",
    "# Fill missing values (if any) with the global mean\n",
    "test['num_sold'] = test['mean_num_sold'].fillna(train['num_sold'].mean())\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('mean_baseline_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the mean_sales \n",
    "print(mean_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "#replace all NaN values in the dataset with the value before it\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['day_of_week'] = train['date'].dt.dayofweek\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['day_of_week'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "\n",
    "# Encode categorical variables\n",
    "train['country'] = train['country'].astype('category')\n",
    "train['store'] = train['store'].astype('category')\n",
    "train['product'] = train['product'].astype('category')\n",
    "\n",
    "test['country'] = test['country'].astype('category')\n",
    "test['store'] = test['store'].astype('category')\n",
    "test['product'] = test['product'].astype('category')\n",
    "\n",
    "# Define features and target\n",
    "features = ['country', 'store', 'product', 'day_of_week', 'month', 'year']\n",
    "X = train[features]\n",
    "y = train['num_sold']\n",
    "X_test = test[features]\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Train LightGBM model\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mape',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 1,\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, valid_sets=[val_data])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "\n",
    "# Predict on test set\n",
    "test['num_sold'] = model.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('lgb_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Replace all NaN values in the dataset with the value before it\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['day_of_week'] = train['date'].dt.dayofweek\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['day_of_week'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "\n",
    "# Encode categorical variables\n",
    "train['country'] = train['country'].astype('category')\n",
    "train['store'] = train['store'].astype('category')\n",
    "train['product'] = train['product'].astype('category')\n",
    "\n",
    "test['country'] = test['country'].astype('category')\n",
    "test['store'] = test['store'].astype('category')\n",
    "test['product'] = test['product'].astype('category')\n",
    "\n",
    "# Define features and target\n",
    "features = ['country', 'store', 'product', 'day_of_week', 'month', 'year']\n",
    "X = train[features]\n",
    "y = train['num_sold']\n",
    "X_test = test[features]\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Convert data to DMatrix (XGBoost's optimized data structure)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, enable_categorical=True)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression objective\n",
    "    'eval_metric': 'mape',           # Evaluation metric (XGBoost doesn't have MAPE natively; calculate later)\n",
    "    'learning_rate': 0.005,\n",
    "    'max_depth': 40,\n",
    "    'colsample_bytree': 1,\n",
    "    'subsample': 1,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "model = xgb.train(params, dtrain, num_boost_round=2000, evals=watchlist, early_stopping_rounds=50)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(dval)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "\n",
    "# Predict on test set\n",
    "test['num_sold'] = model.predict(dtest)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('xgb2_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Replace all NaN values in the dataset with the value before it\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['day_of_week'] = train['date'].dt.dayofweek\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['day_of_week'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "\n",
    "# Encode categorical variables (CatBoost handles categorical variables natively)\n",
    "categorical_features = ['country', 'store', 'product']\n",
    "train[categorical_features] = train[categorical_features].astype('category')\n",
    "test[categorical_features] = test[categorical_features].astype('category')\n",
    "\n",
    "# Define features and target\n",
    "features = ['country', 'store', 'product', 'day_of_week', 'month', 'year']\n",
    "X = train[features]\n",
    "y = train['num_sold']\n",
    "X_test = test[features]\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Initialize CatBoost model\n",
    "model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.01,\n",
    "    depth=15,\n",
    "    loss_function='MAPE',\n",
    "    eval_metric='MAPE',\n",
    "    cat_features=categorical_features,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "\n",
    "# Predict on test set\n",
    "test['num_sold'] = model.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('catboost_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Replace NaN values\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['day_of_week'] = train['date'].dt.dayofweek\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['day_of_week'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "\n",
    "# Define features and target\n",
    "features = ['day_of_week', 'month', 'year']\n",
    "categorical_features = ['country', 'store', 'product']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train = pd.get_dummies(train, columns=categorical_features)\n",
    "test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "X = train[features + list(test.columns.difference(features))]\n",
    "y = train['num_sold']\n",
    "X_test = test[features + list(test.columns.difference(features))]\n",
    "\n",
    "X = X.drop(columns=['date'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['date'], errors='ignore')\n",
    "\n",
    "# Ensure same columns in train and test\n",
    "X_test = X_test.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Linear activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "\n",
    "# Predict on test set\n",
    "test['num_sold'] = model.predict(X_test_scaled)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('nn_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Replace NaN values\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['day_of_week'] = train['date'].dt.dayofweek\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['day_of_week'] = test['date'].dt.dayofweek\n",
    "test['month'] = test['date'].dt.month\n",
    "test['year'] = test['date'].dt.year\n",
    "\n",
    "# Define features and target\n",
    "features = ['day_of_week', 'month', 'year']\n",
    "categorical_features = ['country', 'store', 'product']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train = pd.get_dummies(train, columns=categorical_features)\n",
    "test = pd.get_dummies(test, columns=categorical_features)\n",
    "\n",
    "X = train[features + list(test.columns.difference(features))]\n",
    "y = train['num_sold']\n",
    "X_test = test[features + list(test.columns.difference(features))]\n",
    "\n",
    "X = X.drop(columns=['date'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['date'], errors='ignore')\n",
    "\n",
    "# Ensure same columns in train and test\n",
    "X_test = X_test.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scale features and target\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NeuralNetwork(input_dim).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze()\n",
    "        loss = criterion(predictions, y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch.squeeze())\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_val_tensor).cpu().numpy()\n",
    "    y_val_original = y_scaler.inverse_transform(y_val_tensor.cpu().numpy())\n",
    "    y_val_pred_original = y_scaler.inverse_transform(y_val_pred)\n",
    "    mape = mean_absolute_percentage_error(y_val_original, y_val_pred_original)\n",
    "    print(f\"Validation MAPE: {mape:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor).cpu().numpy()\n",
    "    test['num_sold'] = y_scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test[['id', 'num_sold']]\n",
    "submission.to_csv('pytorch_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the id to index and drop NaNs in target\n",
    "train.set_index(\"id\", inplace=True)\n",
    "test.set_index(\"id\", inplace=True)\n",
    "train.dropna(subset=[\"num_sold\"], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "def process_date_features(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "    # Cyclical features\n",
    "    df['day_sin']    = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos']    = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin']  = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']  = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "    df.drop(\"date\", axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train = process_date_features(train)\n",
    "test = process_date_features(test)\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[\"num_sold\"])\n",
    "y = np.log1p(train[\"num_sold\"])  # Log transform target\n",
    "X_test = test[X.columns]\n",
    "\n",
    "# Encode categorical features\n",
    "cat_cols = [\"country\", \"store\", \"product\", \"day_of_week\"]\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X[col], X_test[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Scale target\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Take the output of the last time step\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate=0.2):\n",
    "        super(DeepRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # out: (batch_size, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]  # Take the last time step's output\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate=0.3):\n",
    "        super(ComplexRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Attention layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Output shape: (batch_size, seq_len, hidden_dim*2)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)  # Shape: (batch_size, hidden_dim*2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc(context_vector)  # Shape: (batch_size, output_dim)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepComplexRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate=0.3):\n",
    "        super(DeepComplexRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Stacked Bidirectional LSTMs\n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_dim if i == 0 else hidden_dim * 2,  # Input size for the first RNN is input_dim, for others it's hidden_dim*2\n",
    "                hidden_dim,\n",
    "                num_layers=1,  # Single layer for each RNN in the stack\n",
    "                batch_first=True,\n",
    "                dropout=dropout_rate,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for rnn in self.rnns:\n",
    "            x, _ = rnn(x)  # Pass through each LSTM layer in the stack\n",
    "\n",
    "        # Attention Mechanism\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.sum(attention_weights * x, dim=1)  # Shape: (batch_size, hidden_dim*2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        out = self.fc(context_vector)  # Shape: (batch_size, output_dim)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN\n",
    "X_train_rnn = X_train_tensor.unsqueeze(1)  # Add sequence dimension (seq_len=1)\n",
    "X_val_rnn = X_val_tensor.unsqueeze(1)\n",
    "X_test_rnn = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_rnn, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_rnn, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_rnn.shape[2]  # Number of features\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "model = DeepComplexRNN(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze()\n",
    "        loss = criterion(predictions, y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch.squeeze())\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    #scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_val_rnn).cpu().numpy()\n",
    "    y_val_original = y_scaler.inverse_transform(y_val_tensor.cpu().numpy())\n",
    "    y_val_pred_original = y_scaler.inverse_transform(y_val_pred)\n",
    "    mape = mean_absolute_percentage_error(y_val_original, y_val_pred_original)\n",
    "    print(f\"Validation MAPE: {mape:.4f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_rnn).cpu().numpy()\n",
    "    test['num_sold'] = y_scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Reset index to make 'id' a column again\n",
    "submission = test.reset_index()[['id', 'num_sold']]\n",
    "submission.to_csv('dcrnn_submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer + LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train shape: torch.Size([189492, 13]), Validation shape: torch.Size([31767, 13])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the id to index and drop NaNs in target\n",
    "train.set_index(\"id\", inplace=True)\n",
    "test.set_index(\"id\", inplace=True)\n",
    "train.dropna(subset=[\"num_sold\"], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "def process_date_features(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "    # Cyclical features\n",
    "    df['day_sin']    = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos']    = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin']  = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']  = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Retain the 'date' column for splitting\n",
    "train = process_date_features(train)\n",
    "test = process_date_features(test)\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[\"num_sold\"])  # Retain 'date' column for now\n",
    "y = np.log1p(train[\"num_sold\"])  # Log transform target\n",
    "X_test = test[X.columns]\n",
    "\n",
    "# Encode categorical features\n",
    "cat_cols = [\"country\", \"store\", \"product\", \"day_of_week\"]\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X[col], X_test[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Split based on date\n",
    "train_data = X[X['date'] < '2016-01-01']\n",
    "val_data = X[X['date'] >= '2016-01-01']\n",
    "train_target = y[X['date'] < '2016-01-01']\n",
    "val_target = y[X['date'] >= '2016-01-01']\n",
    "\n",
    "# Drop the 'date' column after splitting\n",
    "train_data = train_data.drop(columns=[\"date\"])\n",
    "val_data = val_data.drop(columns=[\"date\"])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data)\n",
    "X_val_scaled = scaler.transform(val_data)\n",
    "X_test_scaled = scaler.transform(X_test.drop(columns=[\"date\"]))\n",
    "\n",
    "# Scale target\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(train_target.values.reshape(-1, 1))\n",
    "y_val_scaled = y_scaler.transform(val_target.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Train shape: {X_train_tensor.shape}, Validation shape: {X_val_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "shape of test (98550, 14)\n",
      "shape of X_test (98550, 14)\n",
      "shape of X_test_scaled (98550, 13)\n",
      "shape of X_test_seq after padding: (98550, 31, 13)\n",
      "Train shape: torch.Size([189462, 31, 13]), Validation shape: torch.Size([31737, 31, 13]), Test shape: torch.Size([98550, 31, 13])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the id to index and drop NaNs in target\n",
    "train.set_index(\"id\", inplace=True)\n",
    "test.set_index(\"id\", inplace=True)\n",
    "train.dropna(subset=[\"num_sold\"], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "def process_date_features(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "    # Cyclical features\n",
    "    df['day_sin']    = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos']    = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin']  = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']  = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Retain the 'date' column for splitting\n",
    "train = process_date_features(train)\n",
    "test = process_date_features(test)\n",
    "print(\"shape of test\",test.shape)\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[\"num_sold\"])  # Retain 'date' column for now\n",
    "y = np.log1p(train[\"num_sold\"])  # Log transform target\n",
    "X_test = test[X.columns]\n",
    "print(\"shape of X_test\",X_test.shape)\n",
    "\n",
    "# Encode categorical features using LabelEncoder and OneHotEncoder for high cardinality features\n",
    "cat_cols = [\"country\", \"store\", \"product\", \"day_of_week\"]\n",
    "\n",
    "# Apply LabelEncoder for smaller categorical variables\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X[col], X_test[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Add lag features for the previous week's sales (example)\n",
    "train['num_sold_lag_1'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(1)\n",
    "train['num_sold_lag_2'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(2)\n",
    "train['num_sold_lag_3'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(3)\n",
    "\n",
    "# Add holiday features (for simplicity, mark major holidays as binary)\n",
    "train['holiday'] = train['date'].apply(lambda x: 1 if x.month == 12 and x.day == 25 else 0)  # Example for Christmas\n",
    "\n",
    "# Fill NaN values due to lag creation\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "# Split based on date\n",
    "train_data = X[X['date'] < '2016-01-01']\n",
    "val_data = X[X['date'] >= '2016-01-01']\n",
    "train_target = y[X['date'] < '2016-01-01']\n",
    "val_target = y[X['date'] >= '2016-01-01']\n",
    "\n",
    "# Drop the 'date' column after splitting\n",
    "train_data = train_data.drop(columns=[\"date\"])\n",
    "val_data = val_data.drop(columns=[\"date\"])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data)\n",
    "X_val_scaled = scaler.transform(val_data)\n",
    "X_test_scaled = scaler.transform(X_test.drop(columns=[\"date\"]))\n",
    "print(\"shape of X_test_scaled\",X_test_scaled.shape)\n",
    "\n",
    "# Scale target\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(train_target.values.reshape(-1, 1))\n",
    "y_val_scaled = y_scaler.transform(val_target.values.reshape(-1, 1))\n",
    "\n",
    "# Add seq_len for sequence-based models\n",
    "seq_len = 31  # Example: 7 days of historical data\n",
    "\n",
    "def create_sequences_test(data, seq_len):\n",
    "    \"\"\"\n",
    "    Create sequences for test data, ensuring alignment with original test rows.\n",
    "    Pads the beginning of the data to ensure all rows are covered.\n",
    "    Args:\n",
    "        data (np.ndarray): Feature data.\n",
    "        seq_len (int): Length of each sequence.\n",
    "    Returns:\n",
    "        np.ndarray: Sequences (X).\n",
    "    \"\"\"\n",
    "    padded_data = np.pad(data, ((seq_len - 1, 0), (0, 0)), mode='constant', constant_values=0)\n",
    "    sequences = []\n",
    "    for i in range(len(data)):  # Ensure all original rows are included\n",
    "        seq_x = padded_data[i:i+seq_len]\n",
    "        sequences.append(seq_x)\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "def create_sequences(data, target, seq_len):\n",
    "    \"\"\"\n",
    "    Create sequences of length seq_len for LSTM/Transformer training.\n",
    "    Args:\n",
    "        data (np.ndarray): Feature data.\n",
    "        target (np.ndarray): Target data.\n",
    "        seq_len (int): Length of each sequence.\n",
    "    Returns:\n",
    "        tuple: Sequences (X) and corresponding targets (y).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seq_x = data[i:i+seq_len]\n",
    "        seq_y = target[i+seq_len-1]  # Target corresponds to the last step in the sequence\n",
    "        sequences.append(seq_x)\n",
    "        targets.append(seq_y)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Apply sliding window on training and validation data\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, seq_len)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, seq_len)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "# # Test set: Prepare sequences, but no target (predict future)\n",
    "# X_test_seq, _ = create_sequences(X_test_scaled, np.zeros(len(X_test_scaled)), seq_len)\n",
    "# print(\"shape of X_test_seq\",X_test_seq.shape)\n",
    "# X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "X_test_seq = create_sequences_test(X_test_scaled, seq_len)\n",
    "print(\"shape of X_test_seq after padding:\", X_test_seq.shape)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train shape: {X_train_tensor.shape}, Validation shape: {X_val_tensor.shape}, Test shape: {X_test_tensor.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import time\n",
    "\n",
    "# Define Transformer + LSTM Model\n",
    "class TransformerLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_hidden_dim, transformer_hidden_dim, num_heads, output_dim):\n",
    "        super(TransformerLSTMModel, self).__init__()\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # Transformer Layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=lstm_hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=1,\n",
    "            dim_feedforward=transformer_hidden_dim\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Transformer expects [seq_len, batch_size, features], so transpose the output\n",
    "        lstm_out = lstm_out.permute(1, 0, 2)  # Change shape to [batch_size, seq_len, features]\n",
    "\n",
    "        # Transformer layer\n",
    "        transformer_out = self.transformer(lstm_out, lstm_out)\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        transformer_out = transformer_out[-1, :, :]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(transformer_out))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.0089, Validation Loss: 0.0050\n",
      "Epoch [2/10], Training Loss: 0.0019, Validation Loss: 0.0016\n",
      "Epoch [3/10], Training Loss: 0.0018, Validation Loss: 0.0017\n",
      "Epoch [4/10], Training Loss: 0.0010, Validation Loss: 0.0041\n",
      "Epoch [5/10], Training Loss: 0.0010, Validation Loss: 0.0010\n",
      "Epoch [6/10], Training Loss: 0.0008, Validation Loss: 0.0011\n",
      "Epoch [7/10], Training Loss: 0.0005, Validation Loss: 0.0007\n",
      "Epoch [8/10], Training Loss: 0.0006, Validation Loss: 0.0009\n",
      "Epoch [9/10], Training Loss: 0.0005, Validation Loss: 0.0010\n",
      "Epoch [10/10], Training Loss: 0.0004, Validation Loss: 0.0009\n"
     ]
    }
   ],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare DataLoader\n",
    "def create_dataloader(X_train, y_train, X_val, y_val, batch_size=64):\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Prepare the final prediction function\n",
    "def make_predictions(model, X_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "    return predictions.squeeze().cpu().numpy()\n",
    "\n",
    "# Instantiate model\n",
    "input_dim = X_train_tensor.shape[-1]  # Number of features\n",
    "lstm_hidden_dim = 128\n",
    "transformer_hidden_dim = 256\n",
    "num_heads = 4\n",
    "output_dim = 1  # Predicting num_sold\n",
    "\n",
    "model = TransformerLSTMModel(input_dim, lstm_hidden_dim, transformer_hidden_dim, num_heads, output_dim).to(device)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader, val_loader = create_dataloader(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Update make_predictions to handle batches\n",
    "def make_predictions(model, X_test, batch_size=1024):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(TensorDataset(X_test), batch_size=batch_size, shuffle=False)\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Generate Predictions for Test Set in batches\n",
    "predictions = make_predictions(model, X_test_tensor, batch_size=1024)\n",
    "\n",
    "# Reverse scaling and log transformation\n",
    "predictions = y_scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "\n",
    "# Check size mismatch and handle it\n",
    "if len(predictions) < len(test):\n",
    "    print(f\"Mismatch detected: Test index ({len(test)}) and predictions ({len(predictions)})\")\n",
    "    # Truncate test index if it exceeds predictions\n",
    "    test = test.iloc[:len(predictions)]\n",
    "elif len(predictions) > len(test):\n",
    "    raise ValueError(\"More predictions than test entries—investigate preprocessing steps.\")\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test.index,\n",
    "    'num_sold': np.expm1(predictions.flatten())  # Reverse log transformation\n",
    "})\n",
    "\n",
    "submission.to_csv('submission2.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN + LSTM + Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "shape of test (98550, 14)\n",
      "shape of X_test (98550, 14)\n",
      "shape of X_test_scaled (98550, 13)\n",
      "shape of X_test_seq after padding: (98550, 31, 13)\n",
      "Train shape: torch.Size([189462, 31, 13]), Validation shape: torch.Size([31737, 31, 13]), Test shape: torch.Size([98550, 31, 13])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the id to index and drop NaNs in target\n",
    "train.set_index(\"id\", inplace=True)\n",
    "test.set_index(\"id\", inplace=True)\n",
    "train.dropna(subset=[\"num_sold\"], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "def process_date_features(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "    # Cyclical features\n",
    "    df['day_sin']    = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos']    = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin']  = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']  = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Retain the 'date' column for splitting\n",
    "train = process_date_features(train)\n",
    "test = process_date_features(test)\n",
    "print(\"shape of test\",test.shape)\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[\"num_sold\"])  # Retain 'date' column for now\n",
    "y = np.log1p(train[\"num_sold\"])  # Log transform target\n",
    "X_test = test[X.columns]\n",
    "print(\"shape of X_test\",X_test.shape)\n",
    "\n",
    "# Encode categorical features using LabelEncoder and OneHotEncoder for high cardinality features\n",
    "cat_cols = [\"country\", \"store\", \"product\", \"day_of_week\"]\n",
    "\n",
    "# Apply LabelEncoder for smaller categorical variables\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X[col], X_test[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Add lag features for the previous week's sales (example)\n",
    "train['num_sold_lag_1'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(1)\n",
    "train['num_sold_lag_2'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(2)\n",
    "train['num_sold_lag_3'] = train.groupby(['country', 'store', 'product'])['num_sold'].shift(3)\n",
    "\n",
    "# Add holiday features (for simplicity, mark major holidays as binary)\n",
    "train['holiday'] = train['date'].apply(lambda x: 1 if x.month == 12 and x.day == 25 else 0)  # Example for Christmas\n",
    "\n",
    "# Fill NaN values due to lag creation\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "# Split based on date\n",
    "train_data = X[X['date'] < '2016-01-01']\n",
    "val_data = X[X['date'] >= '2016-01-01']\n",
    "train_target = y[X['date'] < '2016-01-01']\n",
    "val_target = y[X['date'] >= '2016-01-01']\n",
    "\n",
    "# Drop the 'date' column after splitting\n",
    "train_data = train_data.drop(columns=[\"date\"])\n",
    "val_data = val_data.drop(columns=[\"date\"])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data)\n",
    "X_val_scaled = scaler.transform(val_data)\n",
    "X_test_scaled = scaler.transform(X_test.drop(columns=[\"date\"]))\n",
    "print(\"shape of X_test_scaled\",X_test_scaled.shape)\n",
    "\n",
    "# Scale target\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(train_target.values.reshape(-1, 1))\n",
    "y_val_scaled = y_scaler.transform(val_target.values.reshape(-1, 1))\n",
    "\n",
    "# Add seq_len for sequence-based models\n",
    "seq_len = 31  # Example: 7 days of historical data\n",
    "\n",
    "def create_sequences_test(data, seq_len):\n",
    "    \"\"\"\n",
    "    Create sequences for test data, ensuring alignment with original test rows.\n",
    "    Pads the beginning of the data to ensure all rows are covered.\n",
    "    Args:\n",
    "        data (np.ndarray): Feature data.\n",
    "        seq_len (int): Length of each sequence.\n",
    "    Returns:\n",
    "        np.ndarray: Sequences (X).\n",
    "    \"\"\"\n",
    "    padded_data = np.pad(data, ((seq_len - 1, 0), (0, 0)), mode='constant', constant_values=0)\n",
    "    sequences = []\n",
    "    for i in range(len(data)):  # Ensure all original rows are included\n",
    "        seq_x = padded_data[i:i+seq_len]\n",
    "        sequences.append(seq_x)\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "def create_sequences(data, target, seq_len):\n",
    "    \"\"\"\n",
    "    Create sequences of length seq_len for LSTM/Transformer training.\n",
    "    Args:\n",
    "        data (np.ndarray): Feature data.\n",
    "        target (np.ndarray): Target data.\n",
    "        seq_len (int): Length of each sequence.\n",
    "    Returns:\n",
    "        tuple: Sequences (X) and corresponding targets (y).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seq_x = data[i:i+seq_len]\n",
    "        seq_y = target[i+seq_len-1]  # Target corresponds to the last step in the sequence\n",
    "        sequences.append(seq_x)\n",
    "        targets.append(seq_y)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Apply sliding window on training and validation data\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, seq_len)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, seq_len)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "# # Test set: Prepare sequences, but no target (predict future)\n",
    "# X_test_seq, _ = create_sequences(X_test_scaled, np.zeros(len(X_test_scaled)), seq_len)\n",
    "# print(\"shape of X_test_seq\",X_test_seq.shape)\n",
    "# X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "X_test_seq = create_sequences_test(X_test_scaled, seq_len)\n",
    "print(\"shape of X_test_seq after padding:\", X_test_seq.shape)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train shape: {X_train_tensor.shape}, Validation shape: {X_val_tensor.shape}, Test shape: {X_test_tensor.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Define CNN + LSTM + Attention Model\n",
    "class CNN_LSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_hidden_dim, cnn_channels, attention_heads, output_dim):\n",
    "        super(CNN_LSTM_Attention, self).__init__()\n",
    "\n",
    "        # CNN Layer: Apply 1D convolutions to capture local patterns\n",
    "        self.cnn1 = nn.Conv1d(input_dim, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(cnn_channels, cnn_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # LSTM Layer: Capture long-term dependencies\n",
    "        self.lstm = nn.LSTM(cnn_channels, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # Attention Mechanism: Focus on important time steps\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_dim, num_heads=attention_heads)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CNN layers to extract local features\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, seq_len, features] -> [batch_size, features, seq_len]\n",
    "        x = torch.relu(self.cnn1(x))\n",
    "        x = torch.relu(self.cnn2(x))\n",
    "        \n",
    "        # Return to [batch_size, seq_len, features] for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Apply attention mechanism (LSTM output to itself)\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "        # Use output of the last time step\n",
    "        output = attn_output[:, -1, :]\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        x = torch.relu(self.fc1(output))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training loss: 0.0493, Validation loss: 0.1325, Validation MAPE: 21552273.2301\n",
      "Epoch [2/20], Training loss: 0.0046, Validation loss: 0.0143, Validation MAPE: 14469765.6324\n",
      "Epoch [3/20], Training loss: 0.0014, Validation loss: 0.0034, Validation MAPE: 7049178.8403\n",
      "Epoch [4/20], Training loss: 0.0010, Validation loss: 0.0014, Validation MAPE: 3961237.0155\n",
      "Epoch [5/20], Training loss: 0.0008, Validation loss: 0.0015, Validation MAPE: 2775019.9610\n",
      "Epoch [6/20], Training loss: 0.0006, Validation loss: 0.0009, Validation MAPE: 3140964.4561\n",
      "Epoch [7/20], Training loss: 0.0005, Validation loss: 0.0008, Validation MAPE: 1656778.8683\n",
      "Epoch [8/20], Training loss: 0.0004, Validation loss: 0.0008, Validation MAPE: 1444897.0309\n",
      "Epoch [9/20], Training loss: 0.0004, Validation loss: 0.0006, Validation MAPE: 1421975.9203\n",
      "Epoch [10/20], Training loss: 0.0003, Validation loss: 0.0007, Validation MAPE: 1533128.5149\n",
      "Epoch [11/20], Training loss: 0.0003, Validation loss: 0.0006, Validation MAPE: 1369457.0225\n",
      "Epoch [12/20], Training loss: 0.0003, Validation loss: 0.0007, Validation MAPE: 1165250.0197\n",
      "Epoch [13/20], Training loss: 0.0003, Validation loss: 0.0008, Validation MAPE: 858388.0256\n",
      "Epoch [14/20], Training loss: 0.0003, Validation loss: 0.0006, Validation MAPE: 1098057.8865\n",
      "Epoch [15/20], Training loss: 0.0003, Validation loss: 0.0005, Validation MAPE: 859972.5727\n",
      "Epoch [16/20], Training loss: 0.0003, Validation loss: 0.0005, Validation MAPE: 717313.3648\n",
      "Epoch [17/20], Training loss: 0.0003, Validation loss: 0.0005, Validation MAPE: 772338.4177\n",
      "Epoch [18/20], Training loss: 0.0003, Validation loss: 0.0006, Validation MAPE: 998711.9258\n",
      "Epoch [19/20], Training loss: 0.0003, Validation loss: 0.0007, Validation MAPE: 428207.6616\n",
      "Epoch [20/20], Training loss: 0.0002, Validation loss: 0.0006, Validation MAPE: 569784.8011\n"
     ]
    }
   ],
   "source": [
    "# Define MAPE Loss function\n",
    "def mape_loss(y_true, y_pred, eps=1e-8):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / (y_true + eps))) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4):\n",
    "    criterion = nn.MSELoss()  # MAPE loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(targets.squeeze(), outputs.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mape = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(targets.squeeze(), outputs.squeeze())\n",
    "                mape = mape_loss(targets.squeeze(), outputs.squeeze())\n",
    "                val_loss += loss.item()\n",
    "                val_mape += mape.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training loss: {running_loss/len(train_loader):.4f}, Validation loss: {val_loss/len(val_loader):.4f}, Validation MAPE: {val_mape/len(val_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare DataLoader\n",
    "def create_dataloader(X_train, y_train, X_val, y_val, batch_size=64):\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Prediction Function\n",
    "def make_predictions(model, X_test):\n",
    "    model.eval()\n",
    "    X_test = X_test.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "    return predictions.squeeze().cpu().numpy()\n",
    "\n",
    "# Train and Evaluate Model\n",
    "# Assuming X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor are already defined\n",
    "\n",
    "# Instantiate Model\n",
    "input_dim = X_train_tensor.shape[-1]\n",
    "lstm_hidden_dim = 128\n",
    "cnn_channels = 64\n",
    "attention_heads = 4\n",
    "output_dim = 1\n",
    "\n",
    "model = CNN_LSTM_Attention(input_dim, lstm_hidden_dim, cnn_channels, attention_heads, output_dim).to(device)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader, val_loader = create_dataloader(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Train the Model\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Update make_predictions to handle batches\n",
    "def make_predictions(model, X_test, batch_size=1024):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(TensorDataset(X_test), batch_size=batch_size, shuffle=False)\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(\"cuda\")\n",
    "            outputs = model(inputs)\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "    return np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Generate Predictions for Test Set in batches\n",
    "predictions = make_predictions(model, X_test_tensor, batch_size=1024)\n",
    "\n",
    "# Reverse scaling and log transformation\n",
    "predictions = y_scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "\n",
    "# Check size mismatch and handle it\n",
    "if len(predictions) < len(test):\n",
    "    print(f\"Mismatch detected: Test index ({len(test)}) and predictions ({len(predictions)})\")\n",
    "    # Truncate test index if it exceeds predictions\n",
    "    test = test.iloc[:len(predictions)]\n",
    "elif len(predictions) > len(test):\n",
    "    raise ValueError(\"More predictions than test entries—investigate preprocessing steps.\")\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test.index,\n",
    "    'num_sold': np.expm1(predictions.flatten())  # Reverse log transformation\n",
    "})\n",
    "\n",
    "submission.to_csv('submission5.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
